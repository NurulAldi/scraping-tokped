name: Tokopedia Data Pipeline

on:
  schedule:
    - cron: '0 1 * * *'
  workflow_dispatch:
    inputs:
      stage:
        description: 'Stage to run: scrap, transform, upload, all'
        required: false
        default: 'all'

jobs:
  run-pipeline:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.13'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Decode service account
        run: echo "${{ secrets.GCP_KEY_B64 }}" | base64 -d > service-account.json

      - name: Run scraping
        if: >
          github.event_name != 'workflow_dispatch' ||
          github.event.inputs.stage == 'scrap' ||
          github.event.inputs.stage == 'all'
        run: python scripts/scrap.py

      - name: Run transform
        if: >
          github.event_name != 'workflow_dispatch' ||
          github.event.inputs.stage == 'transform' ||
          github.event.inputs.stage == 'all'
        run: python scripts/prep_data.py

      - name: Upload to BigQuery
        if: >
          github.event_name != 'workflow_dispatch' ||
          github.event.inputs.stage == 'upload' ||
          github.event.inputs.stage == 'all'
        run: python scripts/upload_to_bigquery.py
